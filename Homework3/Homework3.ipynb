{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Semantic Retrieval, Text Clustering, and IR Evaluation\n",
    "\n",
    "In the following are my results for the 3rd Homework Assignement for Information Retrieval and Web Search.\n",
    "The notebook is divided into 4 parts:\n",
    "        0. Utility Functions\n",
    "        1. Latent Semantic Indexing\n",
    "        2. Text Clustering\n",
    "        3. IR Evaluation\n",
    "        4. Semantic Retrieval and Word Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import math\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Utilility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Preprocessing of the corpus, filter for nouns and adjectives and lemmatize\"\"\"\n",
    "    # stop = set(stopwords.words('english'))\n",
    "    tags = {'NN', 'NNS', 'NNP', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS'}\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # for i in range(len(query)):\n",
    "    query = [(word.lower(), convert(tag)) for (word, tag) in nltk.pos_tag(nltk.word_tokenize(query)) if tag in tags]\n",
    "    query = [wordnet_lemmatizer.lemmatize(w, t) for (w, t) in query ]\n",
    "    return query\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"Preprocessing of the corpus, filter for nouns and adjectives and lemmatize\"\"\"\n",
    "    # stop = set(stopwords.words('english'))\n",
    "    tags = {'NN', 'NNS', 'NNP', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS'}\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = [(word.lower(), convert(tag)) for (word, tag) in nltk.pos_tag(nltk.word_tokenize(docs[i])) if tag in tags]\n",
    "    return lemmatize_docs(docs)\n",
    "\n",
    "def lemmatize_docs(docs):\n",
    "    \"\"\"Lemmatize the terms of the corpus\"\"\"\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = [wordnet_lemmatizer.lemmatize(w, t) for (w, t) in docs[i]]\n",
    "    return docs\n",
    "\n",
    "def convert(tag):\n",
    "    \"\"\"Convert tag from treebank to wordnet format\"\"\"\n",
    "    if is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    \n",
    "def is_noun(tag):\n",
    "    \"\"\"True if tag corresponds to treebank noun tags\"\"\"\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    \"\"\"True if tag corresponds to treebank adjective tags\"\"\"\n",
    "    return tag in ['JJ', 'JJR', 'JJS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"\"\"Frodo and Sam were trembling in the darkness, surrounded in darkness by hundreds of blood-\n",
    "thirsty orc. Sam was certain these beast were about to taste the scent of their flesh.\"\"\"\n",
    "\n",
    "d2 = \"\"\"The faceless black beast then stabbed Frodo. He felt like every nerve in his body was hurting.\n",
    "Suddenly, he thought of Sam and his calming smile. Frodo had betrayed him.\"\"\"\n",
    "\n",
    "d3 = \"\"\"Frodo’s sword was radiating blue, stronger and stronger every second. Orc were getting\n",
    "closer. And these weren’t just regular orc either, Uruk-Hai were among them. Frodo had\n",
    "killed regular orc before, but he had never stabbed an Uruk-Hai, not with the blue stick.\"\"\"\n",
    "\n",
    "d4 = \"\"\"Sam was carrying a small lamp, shedding some blue light. He was afraid that orc might\n",
    "spot him, but it was the only way to avoid deadly pitfalls of Mordor.\"\"\"\n",
    "\n",
    "docs = [d1, d2, d3, d4]\n",
    "docs = preprocess(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = [word.lower() for word in tokenizer.tokenize(docs[i])]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Your vocabulary consists of the following terms: Frodo, Sam, beast, orc, and blue.\n",
    "Compute the TF-IDF term-document occurrence matrix for given document collection and\n",
    "vocabulary terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTFIDF(docs):\n",
    "    tf = calcTF(terms, docs)\n",
    "    idf = calcIDF(terms, docs)\n",
    "    return np.multiply(idf, tf)\n",
    "\n",
    "def calcTF(docs):\n",
    "    tf = np.zeros((len(terms), len(docs)))\n",
    "    for i in range(0, len(docs)):\n",
    "        for j in range(0, len(terms)):\n",
    "            tf[j][i] = docs[i].count(terms[j])\n",
    "    return tf\n",
    "\n",
    "def calcIDF(docs):\n",
    "    doc_count = len(docs)\n",
    "    idf = np.zeros((len(terms),len(terms)))\n",
    "    for j in range(0, len(terms)):\n",
    "        term_count = 0\n",
    "        for i in range(0, len(docs)):\n",
    "            if terms[j] in docs[i]:\n",
    "                term_count += 1\n",
    "            idf[j][j] = doc_count / term_count  if term_count > 0 else 0\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.33333333,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  1.33333333,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  2.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.33333333,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  2.        ]])\n",
      "array([[ 1.,  2.,  2.,  0.],\n",
      "       [ 2.,  1.,  0.,  1.],\n",
      "       [ 1.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  3.,  1.],\n",
      "       [ 0.,  0.,  2.,  1.]])\n",
      "array([[ 1.33333333,  2.66666667,  2.66666667,  0.        ],\n",
      "       [ 2.66666667,  1.33333333,  0.        ,  1.33333333],\n",
      "       [ 2.        ,  2.        ,  0.        ,  0.        ],\n",
      "       [ 1.33333333,  0.        ,  4.        ,  1.33333333],\n",
      "       [ 0.        ,  0.        ,  4.        ,  2.        ]])\n"
     ]
    }
   ],
   "source": [
    "terms = [\"Frodo\", \"Sam\", \"beast\",\"orc\", \"blue\"]\n",
    "terms = [t.lower() for t in terms]\n",
    "idf = calcIDF(docs)\n",
    "pprint(idf)\n",
    "tf  = calcTF(docs)\n",
    "pprint(tf)\n",
    "tfidf = np.transpose(np.matmul(np.transpose(tf), idf))\n",
    "pprint(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Perform the singular value decomposition of the above matrix and write down\n",
    "the obtained factor matrices U, Σ, and V. You can use some existing programming library\n",
    "to perform the SVD (e.g., numpy.linalg.svd in Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -4.85279372e-01,  -3.20298863e-01,  -7.05471070e-01,\n",
      "          9.51352972e-02,  -3.93919299e-01],\n",
      "       [ -2.41792427e-01,  -5.50491549e-01,   6.63994195e-01,\n",
      "          2.05997018e-01,  -3.93919299e-01],\n",
      "       [ -1.75793034e-01,  -5.83892823e-01,  -6.20129059e-02,\n",
      "         -6.02554933e-02,   7.87838597e-01],\n",
      "       [ -5.98256394e-01,   2.34463181e-01,   2.05534526e-01,\n",
      "         -7.38154362e-01,   3.05311332e-16],\n",
      "       [ -5.63228595e-01,   4.45492851e-01,   1.23823405e-01,\n",
      "          6.32464953e-01,   2.62612866e-01]]),\n",
      " array([ 7.08256944,  4.34281253,  2.12423778,  0.82886997]),\n",
      " array([[-0.34466026, -0.27787293, -0.83868126, -0.31719029],\n",
      "       [-0.6332787 , -0.63458983,  0.42960496,  0.10813604],\n",
      "       [ 0.46136234, -0.52722742, -0.26542436,  0.66236391],\n",
      "       [-0.51702242,  0.49205043, -0.20396868,  0.67005296]])]\n"
     ]
    }
   ],
   "source": [
    "u, sigma, v = np.linalg.svd(tfidf)\n",
    "pprint([u, sigma, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "Reduce the rank of the factor matrices to K = 2, i.e., compute the 2-dimensional\n",
    "vectors for vocabulary terms and documents. Show terms and documents as points in a\n",
    "2-dimensional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_2 = u[:,0:2]\n",
    "sigma_2 = np.diag(sigma[0:2])\n",
    "v_2 = np.transpose(v[:,0:2])\n",
    "dense_vector = np.matmul(sigma_2, v_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADThJREFUeJzt3V+IXHcZxvFnd2adpe7ssjHT9qJqsWFeKRRZKBixmhJS\ntYJorRIJLRqqEqHQP5RiS3NXKV7UGtCC/yqlpSStWhGkIm2sgm01lW3Rm3dZQSHedJqs+8c04+5k\nvJjJmqZndzZnT+acd/b7gULO+c3MeaCbJ7/9nTPnDLXbbQEAYhnOOwAA4MJR3gAQEOUNAAFR3gAQ\nEOUNAAGV+3WgRmOxEJe1TE5eorm5U3nHSFTkbFKx85EtHbKl089stVp1KGn/lpt5l8ulvCOsqcjZ\npGLnI1s6ZEunCNm2XHkDwCCgvAEgIMobAAKivAEgIMobAAKivDegudzSG3On1Fxu5R0FACT18Trv\niFpnzujI0VlNzzR0cqGpbeMVTdVr2rt7h0rD/LsHID+U9zqOHJ3V868eX90+sdBc3d63p55XLABg\n2WQtzeWWpmcaiWPTM2+yhAIgV5T3GuaXmjq50Ewcm1s8rfml5DEA6AfKew0TYxVtG68kjk1WRzUx\nljwGAP1Aea+hMlLSVL2WODZV367KSP73NgCwdaU6YWlmI5Iek3SlpIqkB939VxnmKoS9u3dI6qxx\nzy2e1mR1VFP17av7ASAvaa82uUXSCXe/1cy2SXpN0sCVd2l4WPv21HXzrqs0v9TUxFiFGTeAQhhK\n8/R4MxuTNOTui2b2HknH3P0D671nZaXVLsJtFAEgmMT7eaeaebv7kiSZWVXSzyQ90Os9Rbmpeq1W\nVaOxmHeMREXOJhU7H9nSIVs6/cxWq1UT96c+YWlm75X0O0lPuPtTaT8HAHDh0p6wvEzSbyXd7u4v\nZBsJANBL2hOW90ualHTQzA52993o7m9lEwsAsJ60a953SLoj4ywAgA3iSzoAEBDlDQABUd4AEBDl\nDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQAB\nUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4AEBDlDQABUd4A\nEBDlDQABUd4AEBDlDQABbaq8zezDZvZiRlkAABtUTvtGM7tX0q2S/pNdHADARmxm5v13SZ/PKggA\nYOOG2u126jeb2ZWSDrv7zl6vXVlptcvlUupjAcAWNZS0M/WyyYWamzvVr0Otq1arqtFYzDtGoiJn\nk4qdj2zpkC2dfmar1aqJ+7naBAACorwBIKBNLZu4+z8k9VzvBgBki5k3AAREeQNAQJQ3AAREeQNA\nQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3\nAAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AAREeQNAQJQ3AARE\neQNAQJQ3AAREeQNAQOU0bzKzYUmPSvqQpKakr7r7bJbBAABrSzvz/pykUXf/iKRvSno4u0gAgF7S\nlvd1kn4jSe7+iqRrM0sEAOgp1bKJpHFJ8+dst8ys7O4ra71hcvISlcullIfLVq1WzTvCmoqcTSp2\nPrKlQ7Z08s6WtrwXJJ2bfHi94pakublTKQ+VrVqtqkZjMe8YiYqcTSp2PrKlQ7Z0+pltrX8k0i6b\n/FHSpyXJzHZK+mvKzwEApJB25v2spBvM7CVJQ5L2ZxcJANBLqvJ29zOSDmScBQCwQXxJBwACorwB\nICDKGwACorwBICDKGwACorwBICDKGwACorwBICDKGwACorwBICDKGwACorwBICDKGwACorwBICDK\nGwACorwBICDKGwACorwBICDKGwACorwBICDKGwACorwBICDKGwACorwBICDKGwACorwBICDKGwAC\norwBICDKGwACorwBICDKGwACorwBIKBNlbeZ3WRmT2UVBgCwMeW0bzSzQ5I+Kem17OIAADZiMzPv\nlyR9I6sgAICNG2q32+u+wMxuk3TXebv3u/sxM7te0gF3/1KvA62stNrlcil1UADYooaSdvZcNnH3\nn0j6yWaPPjd3arMfkYlarapGYzHvGImKnE0qdj6ypUO2dPqZrVarJu7nahMACIjyBoCAUl9tIknu\n/qKkFzNJgouiudzS/FJTE2MVVUY45wAMik2VN4qrdeaMjhyd1fRMQycXmto2XtFUvaa9u3eoNMwv\nXEB0lPeAOnJ0Vs+/enx1+8RCc3V73556XrEAZIQp2ABqLrc0PdNIHJueeVPN5VafEwHIGuU9gOaX\nmjq50Ewcm1s8rfml5DEAcVDeA2hirKJt45XEscnqqCbGkscAxEF5D6DKSElT9Vri2FR9O1edAAOA\nE5YDau/uHZI6a9xzi6c1WR3VVH376n4AsVHeA6o0PKx9e+q6eddVXOcNDCDKe8BVRkq6dPKSvGMA\nyBhr3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR\n3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR3gAQEOUNAAFR3gAQUDnNm8xsQtKT\nksYlvUvS3e7+cpbBAABrSzvzvlvSC+6+S9JXJH0/s0QAgJ5SzbwlPSKpec5nnM4mDgBgI4ba7fa6\nLzCz2yTddd7u/e5+zMwul/ScpDvd/ffrfc7KSqtdLpc2FRYAtqChxJ29ynstZnaNpMOS7nH353q9\nvtFYTHegjNVqVTUai3nHSFTkbFKx85EtHbKl089stVo1sbzTnrC8WtIzkva6++ubCQYAuHBp17wf\nkjQq6ZCZSdK8u382s1QAgHWlKm+KGgDyxZd0ACAgyhsAAqK8ASAgyhsALpLmcktvzJ1Sc7mV+Wen\nvdoEALCG1pkzOnJ0VtMzDZ1caGrbeEVT9Zr27t6h0nA2c2bKGwAyduTorJ5/9fjq9omF5ur2vj31\nTI7BsgkAZKi53NL0TCNxbHrmzcyWUChvAMjQ/FJTJxeaiWNzi6c1v5Q8dqEobwDI0MRYRdvGK4lj\nk9VRTYwlj10oyhsAMlQZKWmqXkscm6pvV2Ukm7urcsISADK2d/cOSZ017rnF05qsjmqqvn11fxYo\nbwDIWGl4WPv21HXzrqs0v9TUxFglsxn3WZQ3AFwklZGSLp285KJ8NmveABAQ5Q0AAVHeABAQ5Q0A\nAVHeABAQ5Q0AAVHeABAQ5Q0AARW+vC/mkygAIKrCfsOyH0+iAICoClve/XgSBQBEVcgpbL+eRAEA\nURWyvPv1JAoAiKqQ5d2vJ1EAQFSFLO9+PYkCAKIq7AnLfjyJAgCiKmx59+NJFAAQVWHL+6yL+SQK\nAIiqkGveAID1pZp5m9m7JT0laVLSfyV92d3/lWUwAMDa0s68vybpL+7+cUlPSro3u0gAgF5Szbzd\n/btmdvbs4fsk/Tu7SACAXoba7fa6LzCz2yTddd7u/e5+zMyOSrpG0g3u/tp6n7Oy0mqXy1wtAgAX\naChxZ6/y7sXMPijp1+5+1aY+CACwYanWvM3sPjO7tbu5JIk7RQFAH6W9zvsxSY93l1RKkvZnFwkA\n0Muml00AAP3Hl3QAICDKGwACorwBICDKGwACKvxdBS+W7vXpf5J0mbufzjuPVOx7xpjZhDq3QhiX\n9C5Jd7v7y/mmeiczu0nSF919X845hiU9KulDkpqSvurus3lmOp+ZfVjSt939+ryznMvMRtS5ou1K\nSRVJD7r7r3IN1dX9ZvmPJJmktqQD7v63PLJsyZm3mY1Lelidv1RFUuR7xtwt6QV33yXpK5K+n2+c\ndzKzQ5IeUjF+rj8nadTdPyLpm+r8vBWGmd0r6ceSRvPOkuAWSSfc/WOSPiXpeznnOddnJMndPyrp\nAUnfyitIEX7I+8rMhiT9UNL9kk7lHOdt3P27+v8PQ9HuGfOIpB90/1yWVIjfVs7zkqRv5B2i6zpJ\nv5Ekd39F0rX5xnmHv0v6fN4h1vCMpIPdPw9JWskxy9u4+y8lfb27+X7l+Hd0oJdN1rgvyz8lHXb3\n180sh1QdG71nTP+T9cx2uTq/FdzZ/2Qd6+Q7YmbX5xApybik+XO2W2ZWdvdCFJG7/9zMrsw7RxJ3\nX5IkM6tK+pk6M9zCcPcVM3tc0k2SvpBXji33JR0zm5V0vLu5U9Kfu8sUhVLEe8aY2TWSDku6x92f\nyztPkm55H3D3L+Wc4zuSXnH3p7vbx939ijwzna9b3ofdfWfeWc5nZu+V9KykR939sbzzJOlOZP4k\n6Wp3/0+/jz/QM+8k7r76BGMz+4ekT+QW5jxmdp+k4+7+hAp2zxgzu1qdX2f3uvvreecJ4I/qrI8+\nbWY7Jf015zxhmNllkn4r6XZ3fyHvPOfq3tPpCnd/SJ1l1zPd//puy5V3wRX5njEPqXNy61B3uWne\n3T+bb6RCe1bSDWb2kjrrtkX6f1l096tzxdVBMzu79n2ju7+VY6azfiHpp2b2B0kjku7MK9eWWzYB\ngEGw5a42AYBBQHkDQECUNwAERHkDQECUNwAERHkDQECUNwAE9D+asZD1bZWfPgAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23aac805be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = dense_vector[:1,:]\n",
    "y = dense_vector[1:2,:]\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23ab48b2160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD3CAYAAAAjdY4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ab48c7978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(dense_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23ab494b198>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD7CAYAAAArZlyJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwafuS9xzoMIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ab48359b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)\n",
    "You are given the query “Sam blue orc”. Compute the latent vector for the query\n",
    "and rank the documents according to similarity of their latent vectors with the obtained\n",
    "latent vector of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_cos_sim(vec1, vec2):\n",
    "#     dotproduct = 0\n",
    "#     logx = 0\n",
    "#     logy = 0\n",
    "#     if len(vec1) != len(vec2):\n",
    "#         raise ValueError('Vectors have different length')\n",
    "#     for i in range(0, len(vec1)):\n",
    "#         dotproduct += vec1[i] * vec2[i]\n",
    "#         logx += vec1[i]**2\n",
    "#         logy += vec2[i]**2\n",
    "#     return dotproduct / (logx * logy)\n",
    "\n",
    "def cosinesim(vec1, vec2):\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError('Vectors have different length')\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in range(0, len(vec1))])\n",
    "    sum1 = sum([vec1[x]**2 for x in range(0,len(vec1))])\n",
    "    sum2 = sum([vec2[x]**2 for x in range(0,len(vec2))])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "                   \n",
    "def calc_similarities(query, docs, sim, k = len(docs)):\n",
    "    ranking = []\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        doc_score = {'doc': i+1,\n",
    "              'similarity': sim(query, doc)}\n",
    "        i += 1\n",
    "        ranking.append(doc_score)\n",
    "    return sorted(ranking, key=lambda x:x['similarity'], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Query TF-IDF Vector\n",
      "array([[ 0.        ],\n",
      "       [ 1.33333333],\n",
      "       [ 0.        ],\n",
      "       [ 1.33333333],\n",
      "       [ 2.        ]])\n",
      "Query Vector in Latent Space\n",
      "array([[-2.24652228],\n",
      "       [ 0.46961454]])\n",
      "[{'doc': 4, 'similarity': 0.948552064568377},\n",
      " {'doc': 1, 'similarity': 0.7867987390042477},\n",
      " {'doc': 2, 'similarity': 0.7268708588672939},\n",
      " {'doc': 3, 'similarity': -0.9190524718049762}]\n"
     ]
    }
   ],
   "source": [
    "query = [\"Sam\", \"blue\", \"orc\"]\n",
    "query = [t.lower() for t in query]\n",
    "\n",
    "tf_query = np.transpose(calcTF([query]))\n",
    "tfidf_query = np.transpose(np.matmul(tf_query, np.transpose(idf)))\n",
    "print(\"Sparse Query TF-IDF Vector\")\n",
    "pprint(tfidf_query)\n",
    "print(\"Query Vector in Latent Space\")\n",
    "dense_query = np.matmul(np.transpose(u_2), tfidf_query)\n",
    "pprint(dense_query)\n",
    "\n",
    "pprint(calc_similarities(np.transpose(dense_query)[0], np.transpose(dense_vector), cosinesim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = [0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24]\n",
    "d2 = [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15]\n",
    "d3 = [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42]\n",
    "d4 = [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38]\n",
    "d5 = [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14]\n",
    "d6 = [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49]\n",
    "d7 = [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21]\n",
    "d8 = [0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38]\n",
    "d9 = [0.4, 0.51, 0.01, 0.1, 0.12, 0.22, 0.26, 0.34, 0.42, 0.38]\n",
    "docs = [d1, d2, d3, d4, d5, d6, d7, d8, d9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Assume that a news outlet is sequentially streaming these documents, from d 1\n",
    "to d 9 . \n",
    "\n",
    "Cluster the documents using the single pass clustering (SPC) algorithm based\n",
    "on cosine similarity between the given TF-IDF document vectors.\n",
    "\n",
    "Run the SPC using different values for similarity threshold: \n",
    "\n",
    "(i) λ = 0.6, (ii) λ = 0.8. \n",
    "\n",
    "What is the difference between the two clusterings, using different values for λ? \n",
    "\n",
    "Next, cluster the documents with SPC assuming the opposite order of streaming, from d 9 to d 1 (use λ = 0.8). \n",
    "\n",
    "Did you obtain the same clusters as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlepassclustering(docs, lambdax):\n",
    "    clusters = {}\n",
    "    result = {}\n",
    "    clusters[0] = []\n",
    "    result[0] = []\n",
    "    clusters[0].append(docs[0])\n",
    "    result[0].append(1)\n",
    "    cluster_count = 0\n",
    "    for i in range(1, len(docs)):\n",
    "        cluster_sim = {}\n",
    "        for c in range(0, len(clusters)):\n",
    "            cluster_sim[c] = simDocCluster(docs[i], clusters[c])\n",
    "        x = max(cluster_sim.keys(), key=(lambda key: cluster_sim[key]))\n",
    "        if(cluster_sim[x] > lambdax):\n",
    "            clusters[x].append(docs[i])\n",
    "            result[x].append(i+1)\n",
    "        else:\n",
    "            cluster_count +=1\n",
    "            clusters[cluster_count] = []\n",
    "            clusters[cluster_count].append(docs[i])\n",
    "            result[cluster_count] = []\n",
    "            result[cluster_count].append(i+1)\n",
    "    return result\n",
    "\n",
    "def simDocCluster(doc, cluster):\n",
    "    similarities = 0.0\n",
    "    for c in cluster:\n",
    "        similarities += cosinesim(doc, c)\n",
    "    return similarities / len(cluster)            \n",
    "\n",
    "def cosinesim(doc1, doc2):\n",
    "    numerator = sum([doc1[x] * doc2[x] for x in range(0, len(doc1))])\n",
    "    sum1 = sum([doc1[x]**2 for x in range(0,len(doc1))])\n",
    "    sum2 = sum([doc2[x]**2 for x in range(0,len(doc2))])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i)\n",
      "{0: [1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "(ii)\n",
      "{0: [1], 1: [2, 3, 4, 5, 9], 2: [6, 8], 3: [7]}\n",
      "(iii)\n",
      "{0: [1, 3, 5, 6, 7], 1: [2, 4, 8], 2: [9]}\n"
     ]
    }
   ],
   "source": [
    "lambda1 = 0.6\n",
    "lambda2 = 0.8\n",
    "lambda3 = 0.8\n",
    "spc1 = singlepassclustering(docs, lambda1)\n",
    "spc2 = singlepassclustering(docs, lambda2)\n",
    "docs_reversed = docs[::-1]\n",
    "spc3 = singlepassclustering(docs_reversed, lambda3)\n",
    "\n",
    "print(\"(i)\")\n",
    "pprint(spc1)\n",
    "print(\"(ii)\")\n",
    "pprint(spc2)\n",
    "print(\"(iii)\")\n",
    "pprint(spc3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Cluster the above given documents using the k-means algorithm, with K = 3\n",
    "and using the following initial centroids:\n",
    "\n",
    "r1 = [0.33, 0.33, 0.42, 0.12, 0.2, 0.34, 0.58, 0.19, 0.07, 0.24]\n",
    "\n",
    "r2 = [0.29, 0.16, 0.38, 0.48, 0.43, 0.11, 0.12, 0.33, 0.03, 0.44]\n",
    "\n",
    "r3 = [0.01, 0.17, 0.11, 0.27, 0.23, 0.37, 0.35, 0.48, 0.54, 0.24].\n",
    "\n",
    "Use the cosine similarity between document vectors and centroids to guide the clustering.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 0, 0, 1, 0, 0, 2, 0, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevelopmentTools\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "r1 = [0.33, 0.33, 0.42, 0.12, 0.2, 0.34, 0.58, 0.19, 0.07, 0.24]\n",
    "r2 = [0.29, 0.16, 0.38, 0.48, 0.43, 0.11, 0.12, 0.33, 0.03, 0.44]\n",
    "r3 = [0.01, 0.17, 0.11, 0.27, 0.23, 0.37, 0.35, 0.48, 0.54, 0.24]\n",
    "centroids = np.array((r1,r2,r3))\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, init=centroids).fit(docs)\n",
    "pprint(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Cluster 0 = d2, d4, d5, d7, d8 \n",
    "\n",
    "Cluster 1 = d6, d9\n",
    "\n",
    "Cluster 2 = d1, d3, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. IR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1 = set(['d1', 'd2', 'd5', 'd6', 'd13'])\n",
    "r2 = set(['d1', 'd2', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10', 'd11', 'd12', 'd13', 'd19', 'd14', 'd17', 'd3', 'd15', 'd16', 'd18', 'd20'])\n",
    "r3 = set(['d1', 'd2', 'd4', 'd5', 'd9', 'd10', 'd12', 'd13', 'd14', 'd15', 'd20'])\n",
    "relevant = set(['d1','d3','d5','d7','d9','d11', 'd13', 'd15', 'd17', 'd19'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Compute the precision, recall and F 1 score for each of the three IR systems.\n",
    "\n",
    "What is the downside of using precision, recall, and F measure to evaluate IR systems?\n",
    "\n",
    "For some query q all odd documents are considered to be relevant and all documents with even identifiers are considered not relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "f-measure = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_measures(retrieved, relevant):\n",
    "    def calc_precision():\n",
    "        tp = len(set.intersection(retrieved, relevant))\n",
    "        return tp/len(retrieved)\n",
    "    \n",
    "    precision = calc_precision()\n",
    "    \n",
    "    def calc_recall():\n",
    "        tp = len(set.intersection(retrieved, relevant))\n",
    "        return tp/len(relevant)\n",
    "    \n",
    "    recall = calc_recall()\n",
    "    \n",
    "    def calc_f1():\n",
    "        return (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "    return (calc_precision(), calc_recall(), calc_f1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR System 1: 'Precision': 0.6 'Recall': 0.3 'F1': 0.4\n",
      "IR System 2: 'Precision': 0.5 'Recall': 1.0 'F1': 0.6666666666666666\n",
      "IR System 3: 'Precision': 0.45454545454545453 'Recall': 0.5 'F1': 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "(p, r, f) = calc_measures(r1, relevant)\n",
    "print(\"IR System 1: 'Precision': \" + str(p) + \" 'Recall': \" + str(r) + \" 'F1': \" + str(f))\n",
    "(p, r, f) = calc_measures(r2, relevant)\n",
    "print(\"IR System 2: 'Precision': \" + str(p) + \" 'Recall': \" + str(r) + \" 'F1': \" + str(f))\n",
    "(p, r, f) = calc_measures(r3, relevant)\n",
    "print(\"IR System 3: 'Precision': \" + str(p) + \" 'Recall': \" + str(r) + \" 'F1': \" + str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Compute the precision at rank 5 (P@5), R-precision, and average precision (AP)\n",
    "for each of the three IR systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR System 1\n",
      "P@5: 0.6\n",
      "R-precision [R@3]: 0.6666666666666666\n",
      "Average Precision: 0.7555555555555555\n",
      "\n",
      "IR System 2\n",
      "P@5: 0.4\n",
      "R-precision [R@10]: 0.4\n",
      "Average Precision: 0.5226696832579185\n",
      "\n",
      "IR System 3\n",
      "P@5: 0.6\n",
      "R-precision [R@5]: 0.6\n",
      "Average Precision: 0.6200000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"IR System 1\")\n",
    "\n",
    "pat5_1 = 3 / (3 + 2)\n",
    "print(\"P@5: \" + str(pat5_1))\n",
    "r_precision_1 = 2 / (2 + 1)\n",
    "print(\"R-precision [R@3]: \" + str(r_precision_1))\n",
    "ap_1 = (1 / 3) * ((1/1)+(2/3)+(3/5))\n",
    "print(\"Average Precision: \" + str(ap_1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"IR System 2\")\n",
    "\n",
    "pat5_2 = 2 / (2 + 3)\n",
    "print(\"P@5: \" + str(pat5_2))\n",
    "r_precision_2 = 4 / (4 + 6)\n",
    "print(\"R-precision [R@10]: \" + str(r_precision_2))\n",
    "ap_2 = (1 / 10) * ((1/1)+(2/4)+(3/6)+(4/8)+(5/10)+(6/12)+(7/13)+(9/15)+(10/17))  \n",
    "print(\"Average Precision: \" + str(ap_2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"IR System 3\")\n",
    "\n",
    "pat5_3 = 3 / (3 + 2)\n",
    "print(\"P@5: \" + str(pat5_3))\n",
    "r_precision_3 = 3 / (3 + 2)\n",
    "print(\"R-precision [R@5]: \" + str(r_precision_3))\n",
    "ap_3 = (1/5)*((1/1)+(2/4)+(3/5)+(4/8)+(5/10))\n",
    "print(\"Average Precision: \" + str(ap_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "You are given a toy IR system which is being evaluated on five queries.\n",
    "\n",
    "The following are the positions of relevant documents for each of these five queries, in the rankings returned by the toy IR system:\n",
    "\n",
    "• q 1 → [1, 6, 9, 17, 21]\n",
    "\n",
    "• q 2 → [1, 3, 4]\n",
    "\n",
    "• q 3 → [2, 5, 8, 9, 10]\n",
    "\n",
    "• q 4 → [4]\n",
    "\n",
    "• q 5 → [1, 2, 6]\n",
    "\n",
    "Evaluate the performance of this IR system in terms of mean average precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.7608333333333334\n"
     ]
    }
   ],
   "source": [
    "precision_q1 = (1/4) * ((1/1)+(2/3)+(3/4)+(4/5))\n",
    "precision_q2 = (1/2) * ((1/1)+(2/2))\n",
    "precision_q3 = (1/2) * ((1/2)+(2/4))\n",
    "precision_q4 = 0\n",
    "precision_q5 = (1/1) * (1/1)\n",
    "\n",
    "MAP = (1/5) * (precision_q1 + precision_q2 + precision_q3 + precision_q3 + precision_q4 + precision_q5)\n",
    "print(\"MAP: \" + str(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Semantic Retrieval with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Newsgroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroup_data = fetch_20newsgroups(remove=('headers', 'footers'))\n",
    "# remove docs with less than 5 words\n",
    "for doc in newsgroup_data.data:\n",
    "    if len(doc) < 5:\n",
    "        newsgroup_data.data.remove(doc)\n",
    "# selecting 500 random articles\n",
    "np.random.seed(1337)\n",
    "newsgroup_articles = np.random.choice(newsgroup_data.data, 500, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings/glove.6B.50d.txt\", \"r\", encoding='utf8') as lines:\n",
    "    embeddings_vec = {}\n",
    "    for line in lines:\n",
    "        embeddings_vec[str(line.split()[0])] = line.split()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "def corpus_preprocessing(docs):\n",
    "#   \"\"\"Tokenize documents and remove stopwords\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    processed = []\n",
    "    for i in range(len(docs)):\n",
    "        processed.append([word.lower() for word in tokenizer.tokenize(docs[i]) if word not in STOPWORDS])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_idf(docs):\n",
    "    \"\"\"Computes the IDF for a collection of Documents\"\"\"\n",
    "    docs_count = len(docs)\n",
    "    idf = {}\n",
    "    for doc in docs:\n",
    "        for term in doc:\n",
    "            if term in idf:\n",
    "                idf[term] += 1\n",
    "            else:\n",
    "                idf[term] = 1\n",
    "    for k,v in idf.items():\n",
    "        idf[k] = docs_count / v\n",
    "    return idf\n",
    "\n",
    "def doc_tf(doc):\n",
    "    \"\"\"Computes the TF for a Document\"\"\"\n",
    "    tf = {}\n",
    "    for term in doc:\n",
    "        if term not in tf:\n",
    "            tf[term] = doc.count(term)\n",
    "    return tf\n",
    "\n",
    "def doc_tfidf(doc, idf):\n",
    "    \"\"\"Computes the TF-IDF for each document of a collection\"\"\"\n",
    "    tfidf = {}\n",
    "    tf = doc_tf(doc)\n",
    "    for term, tf in tf.items():\n",
    "        tfidf[term] = idf[term] * tf\n",
    "    return tfidf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_articles_processed = corpus_preprocessing(newsgroup_articles)\n",
    "nap_idf = docs_idf(newsgroup_articles_processed)\n",
    "nap_tfidf = [doc_tfidf(doc, nap_idf) for doc in newsgroup_articles_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Embedding Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "def calc_document_embedding_vec(doc, word_embeddings):\n",
    "    \"\"\"Calculates the embedding vector for a given document and embeddings vector\"\"\"\n",
    "    matches = set(doc.keys()).intersection(set(word_embeddings.keys()))\n",
    "    doc_vec = {}\n",
    "    nominator = 0\n",
    "    denominator = 0\n",
    "    for term in matches:\n",
    "        term_idf = doc[term]\n",
    "        term_w2v = sum([float(embedding) for embedding in word_embeddings[term]])\n",
    "        nominator += term_idf * term_w2v\n",
    "        denominator += term_idf\n",
    "        doc_vec[term] = nominator / denominator\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinesim_diff(vec1, vec2):\n",
    "    dotproduct = 0\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    for term in set.union(set(vec1), set(vec2)):\n",
    "        c1 = vec1.get(term, 0)\n",
    "        c2 = vec2.get(term, 0)\n",
    "        sum1 += c1**2\n",
    "        sum2 += c2**2\n",
    "        dotproduct += c1 * c2\n",
    "    return dotproduct / (math.sqrt(sum1) + math.sqrt(sum2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroup_embeddings = [calc_document_embedding_vec(doc, embeddings_vec) for doc in nap_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc': 241, 'similarity': 0.09820105548394405},\n",
      " {'doc': 186, 'similarity': 0.09730761997967266},\n",
      " {'doc': 79, 'similarity': 0.08282667940949827},\n",
      " {'doc': 483, 'similarity': 0.07101421257559107},\n",
      " {'doc': 226, 'similarity': 0.034921365483137234},\n",
      " {'doc': 53, 'similarity': 0.03439517315239352},\n",
      " {'doc': 492, 'similarity': 0.031403798728340364},\n",
      " {'doc': 199, 'similarity': 0.030631675409969326},\n",
      " {'doc': 16, 'similarity': 0.027333314235764468},\n",
      " {'doc': 64, 'similarity': 0.027244578001592183}]\n"
     ]
    }
   ],
   "source": [
    "query = [\"Hello World\"]\n",
    "query = corpus_preprocessing(query)\n",
    "query_tfidf = doc_tfidf(query[0], nap_idf)\n",
    "query_embedding = calc_document_embedding_vec(query_tfidf, embeddings_vec)\n",
    "ranking_embedded = calc_similarities(query=query_embedding, docs=newsgroup_embeddings, sim=cosinesim_diff, k=10)\n",
    "pprint(ranking_embedded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
