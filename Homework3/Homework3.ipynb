{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Semantic Retrieval, Text Clustering, and IR Evaluation\n",
    "\n",
    "In the following are my results for the 3rd Homework Assignement for Information Retrieval and Web Search.\n",
    "The notebook is divided into 4 parts:\n",
    "        0. Utility Functions\n",
    "        1. Latent Semantic Indexing\n",
    "        2. Text Clustering\n",
    "        3. IR Evaluation\n",
    "        4. Semantic Retrieval and Word Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Utilility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Preprocessing of the corpus, filter for nouns and adjectives and lemmatize\"\"\"\n",
    "    # stop = set(stopwords.words('english'))\n",
    "    tags = {'NN', 'NNS', 'NNP', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS'}\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # for i in range(len(query)):\n",
    "    query = [(word.lower(), convert(tag)) for (word, tag) in nltk.pos_tag(nltk.word_tokenize(query)) if tag in tags]\n",
    "    query = [wordnet_lemmatizer.lemmatize(w, t) for (w, t) in query ]\n",
    "    return query\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"Preprocessing of the corpus, filter for nouns and adjectives and lemmatize\"\"\"\n",
    "    # stop = set(stopwords.words('english'))\n",
    "    tags = {'NN', 'NNS', 'NNP', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS'}\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = [(word.lower(), convert(tag)) for (word, tag) in nltk.pos_tag(nltk.word_tokenize(docs[i])) if tag in tags]\n",
    "    return lemmatize_docs(docs)\n",
    "\n",
    "def lemmatize_docs(docs):\n",
    "    \"\"\"Lemmatize the terms of the corpus\"\"\"\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = [wordnet_lemmatizer.lemmatize(w, t) for (w, t) in docs[i]]\n",
    "    return docs\n",
    "\n",
    "def convert(tag):\n",
    "    \"\"\"Convert tag from treebank to wordnet format\"\"\"\n",
    "    if is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1 = \"\"\"Frodo and Sam were trembling in the darkness, surrounded in darkness by hundreds of blood-\n",
    "thirsty orcs. Sam was certain these beasts were about to taste the scent of their flesh.\"\"\"\n",
    "\n",
    "d2 = \"\"\"The faceless black beast then stabbed Frodo. He felt like every nerve in his body was hurting.\n",
    "Suddenly, he thought of Sam and his calming smile. Frodo had betrayed him.\"\"\"\n",
    "\n",
    "d3 = \"\"\"Frodo’s sword was radiating blue, stronger and stronger every second. Orcs were getting\n",
    "closer. And these weren’t just regular orcs either, Uruk-Hai were among them. Frodo had\n",
    "killed regular orcs before, but he had never stabbed an Uruk-Hai, not with the blue stick.\"\"\"\n",
    "\n",
    "d4 = \"\"\"Sam was carrying a small lamp, shedding some blue light. He was afraid that orcs might\n",
    "spot him, but it was the only way to avoid deadly pitfalls of Mordor.\"\"\"\n",
    "\n",
    "docs = [d1, d2, d3, d4]\n",
    "docs = preprocess(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = [word.lower() for word in tokenizer.tokenize(docs[i])]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Your vocabulary consists of the following terms: Frodo, Sam, beast, orc, and blue.\n",
    "Compute the TF-IDF term-document occurrence matrix for given document collection and\n",
    "vocabulary terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.25,  0.5 ,  0.5 ,  0.  ],\n",
      "       [ 0.5 ,  0.25,  0.  ,  0.25],\n",
      "       [ 0.  ,  0.25,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.5 ,  0.25]])\n",
      "array([[ 1.,  2.,  2.,  0.],\n",
      "       [ 2.,  1.,  0.,  1.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 0.,  0.,  2.,  1.]])\n",
      "array([[ 0.25,  1.  ,  1.  ,  0.  ],\n",
      "       [ 1.  ,  0.25,  0.  ,  0.25],\n",
      "       [ 0.  ,  0.25,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  1.  ,  0.25]])\n"
     ]
    }
   ],
   "source": [
    "terms = [\"Frodo\", \"Sam\", \"beast\", \"blue\"]\n",
    "terms = [t.lower() for t in terms]\n",
    "idf = calcIDF(terms, docs)\n",
    "pprint(idf)\n",
    "tf  = calcTF(terms, docs)\n",
    "pprint(tf)\n",
    "tfidf = np.multiply(idf, tf)\n",
    "pprint(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcIDF(terms, docs):\n",
    "    doc_count = len(docs)\n",
    "    idf = np.zeros((len(terms), len(docs)))\n",
    "    for i in range(0, len(docs)):\n",
    "        for j in range(0, len(terms)):\n",
    "            term_count = 0\n",
    "            for t in docs[i]:\n",
    "                if t == terms[j]:\n",
    "                    term_count += 1\n",
    "            idf[j][i] = term_count / doc_count\n",
    "    return idf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcTF(terms, docs):\n",
    "    tf = np.zeros((len(terms), len(docs)))\n",
    "    for i in range(0, len(docs)):\n",
    "        for j in range(0, len(terms)):\n",
    "            tf[j][i] = docs[i].count(terms[j])\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Perform the singular value decomposition of the above matrix and write down\n",
    "the obtained factor matrices U, Σ, and V. You can use some existing programming library\n",
    "to perform the SVD (e.g., numpy.linalg.svd in Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.83156252,  0.05896598, -0.50302182, -0.22802596],\n",
       "        [-0.26524368, -0.91052069,  0.3150165 ,  0.03691153],\n",
       "        [-0.0812533 , -0.04195391, -0.30090625,  0.94925929],\n",
       "        [-0.48119379,  0.40708101,  0.74645099,  0.21342095]]),\n",
       " array([ 1.680796  ,  1.03322629,  0.64419841,  0.06983288]),\n",
       " array([[-0.28149419, -0.54628091, -0.78103251, -0.1110244 ],\n",
       "        [-0.8669729 , -0.17339152,  0.45105994, -0.12181254],\n",
       "        [ 0.2937931 , -0.77537331,  0.37787918,  0.41193345],\n",
       "        [-0.28775792,  0.26515802, -0.20914225,  0.8961842 ]]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.svd(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "Reduce the rank of the factor matrices to K = 2, i.e., compute the 2-dimensional\n",
    "vectors for vocabulary terms and documents. Show terms and documents as points in a\n",
    "2-dimensional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)\n",
    "You are given the query “Sam blue orc”. Compute the latent vector for the query\n",
    "and rank the documents according to similarity of their latent vectors with the obtained\n",
    "latent vector of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = [0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24]\n",
    "d2 = [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15]\n",
    "d3 = [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42]\n",
    "d4 = [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38]\n",
    "d5 = [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14]\n",
    "d6 = [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49]\n",
    "d7 = [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21]\n",
    "d8 = [0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38]\n",
    "d9 = [0.4, 0.51, 0.01, 0.1, 0.12, 0.22, 0.26, 0.34, 0.42, 0.38]\n",
    "docs = [d1, d2, d3, d4, d5, d6, d7, d8, d9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Assume that a news outlet is sequentially streaming these documents, from d 1\n",
    "to d 9 . \n",
    "\n",
    "Cluster the documents using the single pass clustering (SPC) algorithm based\n",
    "on cosine similarity between the given TF-IDF document vectors.\n",
    "\n",
    "Run the SPC using different values for similarity threshold: \n",
    "\n",
    "(i) λ = 0.6, (ii) λ = 0.8. \n",
    "\n",
    "What is the difference between the two clusterings, using different values for λ? \n",
    "\n",
    "Next, cluster the documents with SPC assuming the opposite order of streaming, from d 9 to d 1 (use λ = 0.8). \n",
    "\n",
    "Did you obtain the same clusters as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i)\n",
      "{0: [1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "(ii)\n",
      "{0: [1, 3, 5, 6, 7], 1: [2, 4, 8], 2: [9]}\n",
      "(iii)\n",
      "{0: [1], 1: [2, 3, 4, 5, 9], 2: [6, 8], 3: [7]}\n"
     ]
    }
   ],
   "source": [
    "lambda1 = 0.6\n",
    "lambda2 = 0.8\n",
    "lambda3 = 0.8\n",
    "spc1 = singlepassclustering(docs, lambda1)\n",
    "spc2 = singlepassclustering(docs, lambda2)\n",
    "docs_reversed = docs[::-1]\n",
    "spc3 = singlepassclustering(docs_reversed, lambda3)\n",
    "\n",
    "print(\"(i)\")\n",
    "pprint(spc1)\n",
    "print(\"(ii)\")\n",
    "pprint(spc2)\n",
    "print(\"(iii)\")\n",
    "pprint(spc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def singlepassclustering(docs, lambdax):\n",
    "    clusters = {}\n",
    "    result = {}\n",
    "    clusters[0] = []\n",
    "    result[0] = []\n",
    "    clusters[0].append(docs[0])\n",
    "    result[0].append(1)\n",
    "    cluster_count = 0\n",
    "    for i in range(1, len(docs)):\n",
    "        cluster_sim = {}\n",
    "        for c in range(0, len(clusters)):\n",
    "            cluster_sim[c] = simDocCluster(docs[i], clusters[c])\n",
    "        x = max(cluster_sim.keys(), key=(lambda key: cluster_sim[key]))\n",
    "        if(cluster_sim[x] > lambdax):\n",
    "            clusters[x].append(docs[i])\n",
    "            result[x].append(i+1)\n",
    "        else:\n",
    "            cluster_count +=1\n",
    "            clusters[cluster_count] = []\n",
    "            clusters[cluster_count].append(docs[i])\n",
    "            result[cluster_count] = []\n",
    "            result[cluster_count].append(i+1)\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simDocCluster(doc, cluster):\n",
    "    similarities = 0.0\n",
    "    for c in cluster:\n",
    "        similarities += cosinesim(doc, c)\n",
    "    return similarities / len(cluster)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosinesim(doc1, doc2):\n",
    "    numerator = sum([doc1[x] * doc2[x] for x in range(0, len(doc1))])\n",
    "    sum1 = sum([doc1[x]**2 for x in range(0,len(doc1))])\n",
    "    sum2 = sum([doc2[x]**2 for x in range(0,len(doc2))])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Cluster the above given documents using the k-means algorithm, with K = 3\n",
    "and using the following initial centroids:\n",
    "\n",
    "r1 = [0.33, 0.33, 0.42, 0.12, 0.2, 0.34, 0.58, 0.19, 0.07, 0.24]\n",
    "\n",
    "r2 = [0.29, 0.16, 0.38, 0.48, 0.43, 0.11, 0.12, 0.33, 0.03, 0.44]\n",
    "\n",
    "r3 = [0.01, 0.17, 0.11, 0.27, 0.23, 0.37, 0.35, 0.48, 0.54, 0.24].\n",
    "\n",
    "Use the cosine similarity between document vectors and centroids to guide the clustering.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 0, 2, 0, 0, 1, 0, 0, 1], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "r1 = [0.33, 0.33, 0.42, 0.12, 0.2, 0.34, 0.58, 0.19, 0.07, 0.24]\n",
    "r2 = [0.29, 0.16, 0.38, 0.48, 0.43, 0.11, 0.12, 0.33, 0.03, 0.44]\n",
    "r3 = [0.01, 0.17, 0.11, 0.27, 0.23, 0.37, 0.35, 0.48, 0.54, 0.24]\n",
    "centroids = np.array((r1,r2,r3))\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, init=centroids).fit(docs)\n",
    "pprint(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Cluster 0 = d2, d4, d5, d7, d8 \n",
    "\n",
    "Cluster 1 = d6, d9\n",
    "\n",
    "Cluster 2 = d1, d3, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. IR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1 = ['d1', 'd2', 'd5', 'd6', 'd13']\n",
    "r2 = ['d1', 'd2', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10', 'd11', 'd12', 'd13', 'd19', 'd14', 'd17', 'd3', 'd15', 'd16', 'd18', 'd20']\n",
    "r3 = ['d1', 'd2', 'd4', 'd5', 'd9', 'd10', 'd12', 'd13', 'd14', 'd15', 'd20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Compute the precision, recall and F 1 score for each of the three IR systems.\n",
    "\n",
    "What is the downside of using precision, recall, and F measure to evaluate IR systems?\n",
    "\n",
    "For some query q all odd documents are considered to be relevant and all documents with even identifiers are considered not relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision = tp / (tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "f-measure = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR System 1\n",
      "precision: 0.6\n",
      "recall: 1.0\n",
      "f1: 0.7499999999999999\n",
      "\n",
      "IR System 2\n",
      "precision: 0.5\n",
      "recall: 0.5\n",
      "f1: 0.5\n",
      "\n",
      "IR System 3\n",
      "precision: 0.5\n",
      "recall: 1.0\n",
      "f1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"IR System 1\")\n",
    "tp_1 = 3\n",
    "tn_1 = 0\n",
    "fp_1 = 2\n",
    "fn_1 = 0\n",
    "precision_r1 = tp_1 / (tp_1 + fp_1)\n",
    "print(\"precision: \" + str(precision_r1))\n",
    "recall_r1 = tp_1 / (tp_1 + fn_1)\n",
    "print(\"recall: \" + str(recall_r1))\n",
    "F1_r1 = 2 * ((precision_r1 * recall_r1)/(precision_r1 + recall_r1))\n",
    "print(\"f1: \" + str(F1_r1))\n",
    "print()\n",
    "print(\"IR System 2\")\n",
    "tp_2 = 5\n",
    "tn_2 = 5\n",
    "fp_2 = 5\n",
    "fn_2 = 5\n",
    "precision_r2 = tp_2 / (tp_2 + fp_2)\n",
    "print(\"precision: \" + str(precision_r2))\n",
    "recall_r2 = tp_2 / (tp_2 + fn_2)\n",
    "print(\"recall: \" + str(recall_r2))\n",
    "F1_r2 = 2 * ((precision_r2 * recall_r2)/(precision_r2 + recall_r2))\n",
    "print(\"f1: \" + str(F1_r2))\n",
    "print()\n",
    "print(\"IR System 3\")\n",
    "tp_3 = 5\n",
    "tn_3 = 1\n",
    "fp_3 = 5\n",
    "fn_3 = 0\n",
    "precision_r3 = tp_3 / (tp_3 + fp_3)\n",
    "print(\"precision: \" + str(precision_r3))\n",
    "recall_r3 = tp_3 / (tp_3 + fn_3)\n",
    "print(\"recall: \" + str(recall_r3))\n",
    "F1_r3 = 2 * ((precision_r3 * recall_r3)/(precision_r3 + recall_r3))\n",
    "print(\"f1: \" + str(F1_r3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Compute the precision at rank 5 (P@5), R-precision, and average precision (AP)\n",
    "for each of the three IR systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR System 1\n",
      "P@5: 0.6\n",
      "R-precision [R@3]: 0.6666666666666666\n",
      "Average Precision: 0.7555555555555555\n",
      "\n",
      "IR System 2\n",
      "P@5: 0.4\n",
      "R-precision [R@10]: 0.4\n",
      "Average Precision: 0.5226696832579185\n",
      "\n",
      "IR System 3\n",
      "P@5: 0.6\n",
      "R-precision [R@5]: 0.6\n",
      "Average Precision: 0.6200000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"IR System 1\")\n",
    "\n",
    "pat5_1 = 3 / (3 + 2)\n",
    "print(\"P@5: \" + str(pat5_1))\n",
    "r_precision_1 = 2 / (2 + 1)\n",
    "print(\"R-precision [R@3]: \" + str(r_precision_1))\n",
    "ap_1 = (1 / 3) * ((1/1)+(2/3)+(3/5))\n",
    "print(\"Average Precision: \" + str(ap_1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"IR System 2\")\n",
    "\n",
    "pat5_2 = 2 / (2 + 3)\n",
    "print(\"P@5: \" + str(pat5_2))\n",
    "r_precision_2 = 4 / (4 + 6)\n",
    "print(\"R-precision [R@10]: \" + str(r_precision_2))\n",
    "ap_2 = (1 / 10) * ((1/1)+(2/4)+(3/6)+(4/8)+(5/10)+(6/12)+(7/13)+(9/15)+(10/17))  \n",
    "print(\"Average Precision: \" + str(ap_2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"IR System 3\")\n",
    "\n",
    "pat5_3 = 3 / (3 + 2)\n",
    "print(\"P@5: \" + str(pat5_3))\n",
    "r_precision_3 = 3 / (3 + 2)\n",
    "print(\"R-precision [R@5]: \" + str(r_precision_3))\n",
    "ap_3 = (1/5)*((1/1)+(2/4)+(3/5)+(4/8)+(5/10))\n",
    "print(\"Average Precision: \" + str(ap_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "You are given a toy IR system which is being evaluated on five queries.\n",
    "\n",
    "The following are the positions of relevant documents for each of these five queries, in the rankings returned by the toy IR system:\n",
    "\n",
    "• q 1 → [1, 6, 9, 17, 21]\n",
    "\n",
    "• q 2 → [1, 3, 4]\n",
    "\n",
    "• q 3 → [2, 5, 8, 9, 10]\n",
    "\n",
    "• q 4 → [4]\n",
    "\n",
    "• q 5 → [1, 2, 6]\n",
    "\n",
    "Evaluate the performance of this IR system in terms of mean average precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.7608333333333334\n"
     ]
    }
   ],
   "source": [
    "precision_q1 = (1/4) * ((1/1)+(2/3)+(3/4)+(4/5))\n",
    "precision_q2 = (1/2) * ((1/1)+(2/2))\n",
    "precision_q3 = (1/2) * ((1/2)+(2/4))\n",
    "precision_q4 = 0\n",
    "precision_q5 = (1/1) * (1/1)\n",
    "\n",
    "MAP = (1/5) * (precision_q1 + precision_q2 + precision_q3 + precision_q3 + precision_q4 + precision_q5)\n",
    "print(\"MAP: \" + str(MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Semantic Retrieval with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
